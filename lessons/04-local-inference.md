# Local AI Inference

> ðŸš§ **Coming Soon** â€” This lesson is under development.

## Planned Content

This lesson will cover:

- **AI model deployment** on Kubernetes
- **Local inference servers** (vLLM, Ollama, TGI)
- **GPU integration** for accelerated inference
- **Model optimization** for edge deployment
- **Performance tuning** and benchmarking

## Prerequisites

- Completed OpenShift deployment
- Basic understanding of AI/ML concepts
- GPU hardware (optional but recommended)

## Estimated Time

3-4 hours hands-on

---

*Check back soon for detailed content.*
